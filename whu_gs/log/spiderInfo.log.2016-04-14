Wed, 13 Apr 2016 16:36:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:36:51 _legacy.py[line:154] CRITICAL Unhandled Error
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 58, in run
    self.crawler_process.start()
  File "/usr/lib/python2.7/dist-packages/scrapy/crawler.py", line 251, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 1194, in run
    self.mainLoop()
  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 1203, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 825, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/utils/reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/engine.py", line 105, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "/usr/lib/python2.7/dist-packages/scrapy/core/engine.py", line 132, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "build/bdist.linux-x86_64/egg/scrapy_redis/scheduler.py", line 82, in next_request
    
  File "build/bdist.linux-x86_64/egg/scrapy_redis/queue.py", line 92, in pop
    
  File "/usr/local/lib/python2.7/dist-packages/rediscluster/pipeline.py", line 435, in multi
    raise RedisClusterException("method multi() is not implemented")
rediscluster.exceptions.RedisClusterException: method multi() is not implemented

Wed, 13 Apr 2016 16:37:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:38:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:39:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:39:20 _legacy.py[line:154] CRITICAL Unhandled Error
Traceback (most recent call last):
  File "/usr/lib/python2.7/dist-packages/scrapy/commands/crawl.py", line 58, in run
    self.crawler_process.start()
  File "/usr/lib/python2.7/dist-packages/scrapy/crawler.py", line 251, in start
    reactor.run(installSignalHandlers=False)  # blocking call
  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 1194, in run
    self.mainLoop()
  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 1203, in mainLoop
    self.runUntilCurrent()
--- <exception caught here> ---
  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 825, in runUntilCurrent
    call.func(*call.args, **call.kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/utils/reactor.py", line 41, in __call__
    return self._func(*self._a, **self._kw)
  File "/usr/lib/python2.7/dist-packages/scrapy/core/engine.py", line 105, in _next_request
    if not self._next_request_from_scheduler(spider):
  File "/usr/lib/python2.7/dist-packages/scrapy/core/engine.py", line 132, in _next_request_from_scheduler
    request = slot.scheduler.next_request()
  File "/home/zhangrui/whu_gs/whu_gs/scrapy_redis/scheduler.py", line 82, in next_request
    request = self.queue.pop(block_pop_timeout)
  File "/home/zhangrui/whu_gs/whu_gs/scrapy_redis/queue.py", line 92, in pop
    pipe.multi()
  File "/usr/local/lib/python2.7/dist-packages/rediscluster/pipeline.py", line 435, in multi
    raise RedisClusterException("method multi() is not implemented")
rediscluster.exceptions.RedisClusterException: method multi() is not implemented

Wed, 13 Apr 2016 16:39:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:40:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:40:46 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:40:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:40:53 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 8590,
 'downloader/request_count': 26,
 'downloader/request_method_count/GET': 26,
 'downloader/response_bytes': 659727,
 'downloader/response_count': 26,
 'downloader/response_status_count/200': 26,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 8, 40, 53, 870144),
 'log_count/DEBUG': 27,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 26,
 'scheduler/dequeued/redis': 26,
 'scheduler/enqueued/redis': 26,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 13, 8, 40, 46, 308375)}
Wed, 13 Apr 2016 16:41:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:41:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:42:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:42:42 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:42:42 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 251,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 17243,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 8, 42, 42, 461545),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 13, 8, 42, 42, 343)}
Wed, 13 Apr 2016 16:42:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:43:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:43:31 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:43:38 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 8959,
 'downloader/request_count': 27,
 'downloader/request_method_count/GET': 27,
 'downloader/response_bytes': 577682,
 'downloader/response_count': 27,
 'downloader/response_status_count/200': 27,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 8, 43, 38, 408983),
 'log_count/DEBUG': 28,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 27,
 'scheduler/dequeued/redis': 27,
 'scheduler/enqueued/redis': 27,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2016, 4, 13, 8, 43, 31, 169585)}
Wed, 13 Apr 2016 16:43:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:44:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:44:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:45:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:45:29 __init__.py[line:145] WARNING /home/zhangrui/下载/pycharm-community-5.0.1/helpers/pydev/pydevd_resolver.py:191: ScrapyDeprecationWarning: `Settings.defaults` attribute is deprecated and won't be supported in Scrapy 0.26, use `Settings.set(name, value, priority='default')` instead
  attr = getattr(var, n)

Wed, 13 Apr 2016 16:45:29 __init__.py[line:133] WARNING /home/zhangrui/下载/pycharm-community-5.0.1/helpers/pydev/pydevd_resolver.py:191: ScrapyDeprecationWarning: `Settings.overrides` attribute is deprecated and won't be supported in Scrapy 0.26, use `Settings.set(name, value, priority='cmdline')` instead
  attr = getattr(var, n)

Wed, 13 Apr 2016 16:45:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:46:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:46:28 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:46:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:47:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:47:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:48:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:48:45 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:48:45 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 251,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 17243,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 8, 48, 45, 681467),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 13, 8, 48, 45, 271568)}
Wed, 13 Apr 2016 16:48:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:49:15 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:49:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:49:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:49:57 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:49:57 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 251,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 17243,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 8, 49, 57, 507739),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 13, 8, 49, 57, 72002)}
Wed, 13 Apr 2016 16:50:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:50:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:51:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:51:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:52:09 __init__.py[line:145] WARNING /home/zhangrui/下载/pycharm-community-5.0.1/helpers/pydev/pydevd_resolver.py:191: ScrapyDeprecationWarning: `Settings.defaults` attribute is deprecated and won't be supported in Scrapy 0.26, use `Settings.set(name, value, priority='default')` instead
  attr = getattr(var, n)

Wed, 13 Apr 2016 16:52:09 __init__.py[line:133] WARNING /home/zhangrui/下载/pycharm-community-5.0.1/helpers/pydev/pydevd_resolver.py:191: ScrapyDeprecationWarning: `Settings.overrides` attribute is deprecated and won't be supported in Scrapy 0.26, use `Settings.set(name, value, priority='cmdline')` instead
  attr = getattr(var, n)

Wed, 13 Apr 2016 16:52:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:52:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:53:15 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:53:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:53:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:54:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:54:22 logstats.py[line:47] INFO Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:54:51 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:55:20 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 16:55:32 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2016, 4, 13, 8, 55, 32, 885311),
 'log_count/CRITICAL': 1,
 'log_count/DEBUG': 1,
 'log_count/INFO': 19,
 'start_time': datetime.datetime(2016, 4, 13, 8, 36, 51, 496175)}
Wed, 13 Apr 2016 16:55:33 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2016, 4, 13, 8, 55, 33, 636102),
 'log_count/CRITICAL': 1,
 'log_count/DEBUG': 1,
 'log_count/INFO': 17,
 'start_time': datetime.datetime(2016, 4, 13, 8, 39, 20, 873527)}
Wed, 13 Apr 2016 17:04:07 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:04:22 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 18072,
 'downloader/request_count': 52,
 'downloader/request_method_count/GET': 52,
 'downloader/response_bytes': 1219238,
 'downloader/response_count': 52,
 'downloader/response_status_count/200': 52,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 9, 4, 22, 955677),
 'log_count/DEBUG': 53,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 52,
 'scheduler/dequeued/redis': 52,
 'scheduler/enqueued/redis': 52,
 'spider_exceptions/IndexError': 5,
 'start_time': datetime.datetime(2016, 4, 13, 9, 4, 7, 197712)}
Wed, 13 Apr 2016 17:11:34 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:11:36 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:11:40 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 5323,
 'downloader/request_count': 17,
 'downloader/request_method_count/GET': 17,
 'downloader/response_bytes': 423606,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 9, 11, 40, 372270),
 'log_count/DEBUG': 18,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 17,
 'scheduler/dequeued/redis': 17,
 'scheduler/enqueued/redis': 27,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2016, 4, 13, 9, 11, 34, 58357)}
Wed, 13 Apr 2016 17:11:47 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 12502,
 'downloader/request_count': 36,
 'downloader/request_method_count/GET': 36,
 'downloader/response_bytes': 813455,
 'downloader/response_count': 36,
 'downloader/response_status_count/200': 36,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 9, 11, 47, 7240),
 'log_count/DEBUG': 38,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 36,
 'scheduler/dequeued/redis': 36,
 'scheduler/enqueued/redis': 26,
 'spider_exceptions/IndexError': 4,
 'start_time': datetime.datetime(2016, 4, 13, 9, 11, 36, 49900)}
Wed, 13 Apr 2016 17:22:29 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:22:31 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:22:39 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 8590,
 'downloader/request_count': 26,
 'downloader/request_method_count/GET': 26,
 'downloader/response_bytes': 612668,
 'downloader/response_count': 26,
 'downloader/response_status_count/200': 26,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 9, 22, 39, 370775),
 'log_count/DEBUG': 27,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 26,
 'scheduler/dequeued/redis': 26,
 'scheduler/enqueued/redis': 27,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2016, 4, 13, 9, 22, 29, 810917)}
Wed, 13 Apr 2016 17:22:40 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 9235,
 'downloader/request_count': 27,
 'downloader/request_method_count/GET': 27,
 'downloader/response_bytes': 624393,
 'downloader/response_count': 27,
 'downloader/response_status_count/200': 27,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 9, 22, 40, 880846),
 'log_count/DEBUG': 29,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 27,
 'scheduler/dequeued/redis': 27,
 'scheduler/enqueued/redis': 26,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 13, 9, 22, 31, 111523)}
Wed, 13 Apr 2016 17:27:26 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:28:32 logstats.py[line:47] INFO Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:29:26 logstats.py[line:47] INFO Crawled 9 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:31:04 logstats.py[line:47] INFO Crawled 16 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:31:36 logstats.py[line:47] INFO Crawled 26 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:31:37 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/exception_count': 8,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 8,
 'downloader/request_bytes': 11954,
 'downloader/request_count': 34,
 'downloader/request_method_count/GET': 34,
 'downloader/response_bytes': 659147,
 'downloader/response_count': 26,
 'downloader/response_status_count/200': 26,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 9, 31, 37, 11385),
 'log_count/DEBUG': 35,
 'log_count/INFO': 5,
 'request_depth_max': 1,
 'response_received_count': 26,
 'scheduler/dequeued/redis': 34,
 'scheduler/enqueued/redis': 34,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 13, 9, 27, 26, 261518)}
Wed, 13 Apr 2016 17:33:15 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:33:27 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 18072,
 'downloader/request_count': 52,
 'downloader/request_method_count/GET': 52,
 'downloader/response_bytes': 1219238,
 'downloader/response_count': 52,
 'downloader/response_status_count/200': 52,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 9, 33, 27, 817150),
 'log_count/DEBUG': 53,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 52,
 'scheduler/dequeued/redis': 52,
 'scheduler/enqueued/redis': 52,
 'spider_exceptions/IndexError': 5,
 'start_time': datetime.datetime(2016, 4, 13, 9, 33, 15, 199197)}
Wed, 13 Apr 2016 17:35:33 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 17:35:41 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 8590,
 'downloader/request_count': 26,
 'downloader/request_method_count/GET': 26,
 'downloader/response_bytes': 659727,
 'downloader/response_count': 26,
 'downloader/response_status_count/200': 26,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 9, 35, 41, 890998),
 'log_count/DEBUG': 27,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 26,
 'scheduler/dequeued/redis': 26,
 'scheduler/enqueued/redis': 26,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 13, 9, 35, 33, 734886)}
Wed, 13 Apr 2016 20:17:13 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 20:17:14 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 251,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 17243,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 12, 17, 14, 19723),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 13, 12, 17, 13, 434777)}
Wed, 13 Apr 2016 20:18:21 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 20:19:21 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 5730,
 'downloader/request_count': 18,
 'downloader/request_method_count/GET': 18,
 'downloader/response_bytes': 440656,
 'downloader/response_count': 18,
 'downloader/response_status_count/200': 18,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 12, 19, 21, 81121),
 'log_count/DEBUG': 19,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 18,
 'scheduler/dequeued/redis': 18,
 'scheduler/enqueued/redis': 18,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2016, 4, 13, 12, 18, 21, 147480)}
Wed, 13 Apr 2016 20:21:24 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 20:21:25 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 20:21:27 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 251,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 17243,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 12, 21, 27, 108620),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 13, 12, 21, 25, 803602)}
Wed, 13 Apr 2016 20:21:28 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 3421,
 'downloader/request_count': 11,
 'downloader/request_method_count/GET': 11,
 'downloader/response_bytes': 274526,
 'downloader/response_count': 11,
 'downloader/response_status_count/200': 11,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 12, 21, 28, 754878),
 'log_count/DEBUG': 12,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 11,
 'scheduler/dequeued/redis': 11,
 'scheduler/enqueued/redis': 11,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 13, 12, 21, 24, 297560)}
Wed, 13 Apr 2016 20:22:46 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 20:22:48 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Wed, 13 Apr 2016 20:23:01 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 9961,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 737764,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 12, 23, 1, 543977),
 'log_count/DEBUG': 31,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 29,
 'scheduler/dequeued/redis': 29,
 'scheduler/enqueued/redis': 27,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 13, 12, 22, 48, 837988)}
Wed, 13 Apr 2016 20:23:01 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 12264,
 'downloader/request_count': 36,
 'downloader/request_method_count/GET': 36,
 'downloader/response_bytes': 971824,
 'downloader/response_count': 36,
 'downloader/response_status_count/200': 36,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 13, 12, 23, 1, 862082),
 'log_count/DEBUG': 37,
 'log_count/INFO': 1,
 'request_depth_max': 3,
 'response_received_count': 36,
 'scheduler/dequeued/redis': 36,
 'scheduler/enqueued/redis': 38,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2016, 4, 13, 12, 22, 46, 838547)}
Thu, 14 Apr 2016 13:49:44 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 13:49:45 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 13:49:57 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 10324,
 'downloader/request_count': 30,
 'downloader/request_method_count/GET': 30,
 'downloader/response_bytes': 696964,
 'downloader/response_count': 30,
 'downloader/response_status_count/200': 30,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 5, 49, 57, 541152),
 'log_count/DEBUG': 32,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 30,
 'scheduler/dequeued/redis': 30,
 'scheduler/enqueued/redis': 27,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 14, 5, 49, 45, 854626)}
Thu, 14 Apr 2016 13:49:57 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 11901,
 'downloader/request_count': 35,
 'downloader/request_method_count/GET': 35,
 'downloader/response_bytes': 944597,
 'downloader/response_count': 35,
 'downloader/response_status_count/200': 35,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 5, 49, 57, 997035),
 'log_count/DEBUG': 36,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 35,
 'scheduler/dequeued/redis': 35,
 'scheduler/enqueued/redis': 38,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2016, 4, 14, 5, 49, 44, 637120)}
Thu, 14 Apr 2016 16:03:22 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:03:23 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:03:26 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621006_0_10002' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:03:26 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621006_0_10003' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:03:26 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621006_0_10004' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:03:26 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621006_0_10005' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:03:27 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621007_0_10006' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:03:27 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621007_0_10007' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:03:34 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621014_0_10025' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:05:41 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:05:42 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:05:50 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 7864,
 'downloader/request_count': 24,
 'downloader/request_method_count/GET': 24,
 'downloader/response_bytes': 581713,
 'downloader/response_count': 24,
 'downloader/response_status_count/200': 24,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 5, 50, 388365),
 'log_count/DEBUG': 25,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 24,
 'scheduler/dequeued/redis': 24,
 'scheduler/enqueued/redis': 27,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 14, 8, 5, 41, 558699)}
Thu, 14 Apr 2016 16:05:52 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 9961,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 655591,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 5, 52, 219162),
 'log_count/DEBUG': 31,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 29,
 'scheduler/dequeued/redis': 29,
 'scheduler/enqueued/redis': 26,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 14, 8, 5, 42, 653274)}
Thu, 14 Apr 2016 16:10:07 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:10:12 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 21846,
 'downloader/request_count': 74,
 'downloader/request_method_count/GET': 74,
 'downloader/response_bytes': 836005,
 'downloader/response_count': 74,
 'downloader/response_status_count/200': 74,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 10, 12, 480214),
 'log_count/DEBUG': 75,
 'log_count/INFO': 1,
 'request_depth_max': 4,
 'response_received_count': 72,
 'scheduler/dequeued/redis': 74,
 'scheduler/enqueued/redis': 74,
 'spider_exceptions/AttributeError': 1,
 'spider_exceptions/UnboundLocalError': 51,
 'start_time': datetime.datetime(2016, 4, 14, 8, 10, 7, 615781)}
Thu, 14 Apr 2016 16:12:24 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:12:26 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:12:26 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621546_0_10012' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:12:26 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621546_0_10013' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:12:28 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 13500,
 'downloader/request_count': 46,
 'downloader/request_method_count/GET': 46,
 'downloader/response_bytes': 502526,
 'downloader/response_count': 46,
 'downloader/response_status_count/200': 46,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 12, 28, 14830),
 'log_count/DEBUG': 47,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 44,
 'scheduler/dequeued/redis': 46,
 'scheduler/enqueued/redis': 71,
 'spider_exceptions/UnboundLocalError': 33,
 'start_time': datetime.datetime(2016, 4, 14, 8, 12, 24, 806302)}
Thu, 14 Apr 2016 16:12:35 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:12:35 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 231,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11889,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 12, 35, 516319),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 14, 8, 12, 35, 261586)}
Thu, 14 Apr 2016 16:12:37 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:12:37 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 231,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11892,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 12, 37, 743552),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 14, 8, 12, 37, 256301)}
Thu, 14 Apr 2016 16:13:37 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:13:39 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:13:40 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 7327,
 'downloader/request_count': 25,
 'downloader/request_method_count/GET': 25,
 'downloader/response_bytes': 278721,
 'downloader/response_count': 25,
 'downloader/response_status_count/200': 25,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 13, 40, 852301),
 'log_count/DEBUG': 27,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 25,
 'scheduler/dequeued/redis': 25,
 'scheduler/enqueued/redis': 2,
 'spider_exceptions/UnboundLocalError': 20,
 'start_time': datetime.datetime(2016, 4, 14, 8, 13, 39, 43845)}
Thu, 14 Apr 2016 16:13:41 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 14750,
 'downloader/request_count': 50,
 'downloader/request_method_count/GET': 50,
 'downloader/response_bytes': 569526,
 'downloader/response_count': 50,
 'downloader/response_status_count/200': 50,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 13, 41, 237986),
 'log_count/DEBUG': 51,
 'log_count/INFO': 1,
 'request_depth_max': 4,
 'response_received_count': 48,
 'scheduler/dequeued/redis': 50,
 'scheduler/enqueued/redis': 73,
 'spider_exceptions/AttributeError': 1,
 'spider_exceptions/UnboundLocalError': 31,
 'start_time': datetime.datetime(2016, 4, 14, 8, 13, 37, 741653)}
Thu, 14 Apr 2016 16:15:29 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:15:30 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:15:32 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460621732_0_10002' for key 'PRIMARY'")
Thu, 14 Apr 2016 16:15:42 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 12264,
 'downloader/request_count': 36,
 'downloader/request_method_count/GET': 36,
 'downloader/response_bytes': 985971,
 'downloader/response_count': 36,
 'downloader/response_status_count/200': 36,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 15, 42, 244516),
 'log_count/DEBUG': 37,
 'log_count/INFO': 1,
 'request_depth_max': 3,
 'response_received_count': 36,
 'scheduler/dequeued/redis': 36,
 'scheduler/enqueued/redis': 38,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2016, 4, 14, 8, 15, 29, 4165)}
Thu, 14 Apr 2016 16:26:34 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:26:35 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:26:47 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 9281,
 'downloader/request_count': 27,
 'downloader/request_method_count/GET': 27,
 'downloader/response_bytes': 626286,
 'downloader/response_count': 27,
 'downloader/response_status_count/200': 27,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 26, 47, 617728),
 'log_count/DEBUG': 28,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 27,
 'scheduler/dequeued/redis': 27,
 'scheduler/enqueued/redis': 27,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 14, 8, 26, 35, 222269)}
Thu, 14 Apr 2016 16:26:49 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 12990,
 'downloader/request_count': 38,
 'downloader/request_method_count/GET': 38,
 'downloader/response_bytes': 1015218,
 'downloader/response_count': 38,
 'downloader/response_status_count/200': 38,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 26, 49, 932754),
 'log_count/DEBUG': 39,
 'log_count/INFO': 1,
 'request_depth_max': 3,
 'response_received_count': 38,
 'scheduler/dequeued/redis': 38,
 'scheduler/enqueued/redis': 38,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2016, 4, 14, 8, 26, 34, 139978)}
Thu, 14 Apr 2016 16:27:54 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:27:55 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 251,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 17210,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 27, 55, 770261),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 14, 8, 27, 54, 639823)}
Thu, 14 Apr 2016 16:27:55 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 16:27:56 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 251,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 17210,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 8, 27, 56, 469718),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 14, 8, 27, 55, 907397)}
Thu, 14 Apr 2016 17:24:53 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 17:24:54 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 17:25:05 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 9961,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 681080,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 9, 25, 5, 830277),
 'log_count/DEBUG': 31,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 29,
 'scheduler/dequeued/redis': 29,
 'scheduler/enqueued/redis': 27,
 'spider_exceptions/IndexError': 2,
 'start_time': datetime.datetime(2016, 4, 14, 9, 24, 54, 428763)}
Thu, 14 Apr 2016 17:25:07 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 12264,
 'downloader/request_count': 36,
 'downloader/request_method_count/GET': 36,
 'downloader/response_bytes': 960482,
 'downloader/response_count': 36,
 'downloader/response_status_count/200': 36,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 9, 25, 7, 243715),
 'log_count/DEBUG': 37,
 'log_count/INFO': 1,
 'request_depth_max': 3,
 'response_received_count': 36,
 'scheduler/dequeued/redis': 36,
 'scheduler/enqueued/redis': 38,
 'spider_exceptions/IndexError': 3,
 'start_time': datetime.datetime(2016, 4, 14, 9, 24, 53, 140851)}
Thu, 14 Apr 2016 18:08:53 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 18:08:55 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 18:08:57 pipelines.py[line:171] ERROR (1062, "Duplicate entry '1460628537_0_10021' for key 'PRIMARY'")
Thu, 14 Apr 2016 18:08:58 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 12521,
 'downloader/request_count': 43,
 'downloader/request_method_count/GET': 43,
 'downloader/response_bytes': 498074,
 'downloader/response_count': 43,
 'downloader/response_status_count/200': 43,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 10, 8, 58, 643056),
 'log_count/DEBUG': 44,
 'log_count/INFO': 1,
 'request_depth_max': 2,
 'response_received_count': 41,
 'scheduler/dequeued/redis': 43,
 'scheduler/enqueued/redis': 67,
 'spider_exceptions/AttributeError': 1,
 'spider_exceptions/UnboundLocalError': 32,
 'start_time': datetime.datetime(2016, 4, 14, 10, 8, 53, 981300)}
Thu, 14 Apr 2016 18:09:16 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 18:09:16 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 231,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11890,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 10, 9, 16, 547965),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 14, 10, 9, 16, 263889)}
Thu, 14 Apr 2016 18:09:17 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Thu, 14 Apr 2016 18:09:18 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 231,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 11892,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 4, 14, 10, 9, 18, 10445),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2016, 4, 14, 10, 9, 17, 697331)}
