Mon, 28 Dec 2015 14:43:26 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:45:44 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:46:30 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2877,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 46, 30, 115436),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeEncodeError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 6, 45, 44, 275289)}
Mon, 28 Dec 2015 14:46:53 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:48:09 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:49:27 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:50:05 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:51:17 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:51:18 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2870,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 51, 18, 608539),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeEncodeError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 6, 51, 17, 689971)}
Mon, 28 Dec 2015 14:51:22 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:52:33 logstats.py[line:47] INFO Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:52:33 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2890,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 52, 33, 414886),
 'log_count/DEBUG': 2,
 'log_count/INFO': 2,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/UnicodeEncodeError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 6, 51, 22, 260957)}
Mon, 28 Dec 2015 14:52:38 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:52:47 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 1420,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 9930,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 52, 47, 887496),
 'log_count/DEBUG': 6,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 6, 52, 38, 825225)}
Mon, 28 Dec 2015 14:52:56 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:52:58 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 978,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 9930,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 52, 58, 11545),
 'log_count/DEBUG': 4,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 6, 52, 56, 30424)}
Mon, 28 Dec 2015 14:53:09 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:53:13 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 978,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 9930,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 53, 13, 515728),
 'log_count/DEBUG': 4,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 6, 53, 9, 491355)}
Mon, 28 Dec 2015 14:53:53 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:54:03 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2846,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 54, 3, 7117),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 6, 53, 53, 443562)}
Mon, 28 Dec 2015 14:54:24 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:55:24 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:56:05 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 663,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 56, 5, 550007),
 'log_count/DEBUG': 4,
 'log_count/INFO': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2015, 12, 28, 6, 54, 24, 195411)}
Mon, 28 Dec 2015 14:57:45 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:57:52 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2892,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 57, 52, 700252),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 6, 57, 45, 484293)}
Mon, 28 Dec 2015 14:58:22 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 14:58:27 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2878,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 6, 58, 27, 433060),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 6, 58, 22, 493328)}
Mon, 28 Dec 2015 15:04:58 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:05:25 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2901,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 5, 25, 689964),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 4, 58, 482079)}
Mon, 28 Dec 2015 15:05:37 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:09:13 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:09:34 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2863,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 9, 34, 474388),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 9, 13, 283067)}
Mon, 28 Dec 2015 15:10:15 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:11:26 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:12:46 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:14:27 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:14:32 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2816,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 14, 32, 788852),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 14, 27, 997773)}
Mon, 28 Dec 2015 15:14:37 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:15:37 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:15:58 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:20:02 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:23:47 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:23:59 cursors.py[line:117] WARNING /home/zhangrui/whu_gs/whu_gs/com/crawler/dao/ContentDao.py:25: Warning: Data truncated for column 'pubdate' at row 1
  Content.ip,Content.summary,Content.topictype,Content.updatetime,Content.forumurl,Content.domaintype,Content.postip,Content.postfloor,Content.userid,Content.userpage,Content.topPost,Content.website_name,Content.annexs));

Mon, 28 Dec 2015 15:25:34 logstats.py[line:47] INFO Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:25:34 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2845,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 25, 34, 714021),
 'log_count/DEBUG': 2,
 'log_count/INFO': 2,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 23, 47, 311417)}
Mon, 28 Dec 2015 15:25:42 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:25:43 cursors.py[line:117] WARNING /home/zhangrui/whu_gs/whu_gs/com/crawler/dao/ContentDao.py:25: Warning: Data truncated for column 'pubdate' at row 1
  Content.ip,Content.summary,Content.topictype,Content.updatetime,Content.forumurl,Content.domaintype,Content.postip,Content.postfloor,Content.userid,Content.userpage,Content.topPost,Content.website_name,Content.annexs));

Mon, 28 Dec 2015 15:25:43 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2882,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 25, 43, 405481),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 25, 42, 718122)}
Mon, 28 Dec 2015 15:26:40 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:26:41 cursors.py[line:117] WARNING /home/zhangrui/whu_gs/whu_gs/com/crawler/dao/ContentDao.py:25: Warning: Data truncated for column 'pubdate' at row 1
  Content.ip,Content.summary,Content.topictype,Content.updatetime,Content.forumurl,Content.domaintype,Content.postip,Content.postfloor,Content.userid,Content.userpage,Content.topPost,Content.website_name,Content.annexs));

Mon, 28 Dec 2015 15:26:41 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2814,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 26, 41, 250184),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 26, 40, 734198)}
Mon, 28 Dec 2015 15:26:55 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:27:01 cursors.py[line:117] WARNING /home/zhangrui/whu_gs/whu_gs/com/crawler/dao/ContentDao.py:25: Warning: Data truncated for column 'pubdate' at row 1
  Content.ip,Content.summary,Content.topictype,Content.updatetime,Content.forumurl,Content.domaintype,Content.postip,Content.postfloor,Content.userid,Content.userpage,Content.topPost,Content.website_name,Content.annexs));

Mon, 28 Dec 2015 15:27:27 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2821,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 27, 27, 309807),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 26, 55, 62463)}
Mon, 28 Dec 2015 15:27:59 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:28:05 cursors.py[line:117] WARNING /home/zhangrui/whu_gs/whu_gs/com/crawler/dao/ContentDao.py:25: Warning: Data truncated for column 'pubdate' at row 1
  Content.ip,Content.summary,Content.topictype,Content.updatetime,Content.forumurl,Content.domaintype,Content.postip,Content.postfloor,Content.userid,Content.userpage,Content.topPost,Content.website_name,Content.annexs));

Mon, 28 Dec 2015 15:28:05 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2838,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 28, 5, 229514),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 27, 59, 693356)}
Mon, 28 Dec 2015 15:29:08 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:29:09 cursors.py[line:117] WARNING /home/zhangrui/whu_gs/whu_gs/com/crawler/dao/ContentDao.py:25: Warning: Data truncated for column 'pubdate' at row 1
  Content.ip,Content.summary,Content.topictype,Content.updatetime,Content.forumurl,Content.domaintype,Content.postip,Content.postfloor,Content.userid,Content.userpage,Content.topPost,Content.website_name,Content.annexs));

Mon, 28 Dec 2015 15:29:09 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2816,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 29, 9, 322662),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 29, 8, 802771)}
Mon, 28 Dec 2015 15:30:05 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:30:06 cursors.py[line:117] WARNING /home/zhangrui/whu_gs/whu_gs/com/crawler/dao/ContentDao.py:25: Warning: Data truncated for column 'pubdate' at row 1
  Content.ip,Content.summary,Content.topictype,Content.updatetime,Content.forumurl,Content.domaintype,Content.postip,Content.postfloor,Content.userid,Content.userpage,Content.topPost,Content.website_name,Content.annexs));

Mon, 28 Dec 2015 15:30:06 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2839,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 30, 6, 700209),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 30, 5, 429732)}
Mon, 28 Dec 2015 15:30:42 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:30:43 cursors.py[line:117] WARNING /home/zhangrui/whu_gs/whu_gs/com/crawler/dao/ContentDao.py:25: Warning: Data truncated for column 'pubdate' at row 1
  Content.ip,Content.summary,Content.topictype,Content.updatetime,Content.forumurl,Content.domaintype,Content.postip,Content.postfloor,Content.userid,Content.userpage,Content.topPost,Content.website_name,Content.annexs));

Mon, 28 Dec 2015 15:30:53 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2852,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 30, 53, 937124),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 30, 42, 943524)}
Mon, 28 Dec 2015 15:31:04 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:31:09 cursors.py[line:117] WARNING /home/zhangrui/whu_gs/whu_gs/com/crawler/dao/ContentDao.py:25: Warning: Data truncated for column 'pubdate' at row 1
  Content.ip,Content.summary,Content.topictype,Content.updatetime,Content.forumurl,Content.domaintype,Content.postip,Content.postfloor,Content.userid,Content.userpage,Content.topPost,Content.website_name,Content.annexs));

Mon, 28 Dec 2015 15:31:09 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2793,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 31, 9, 841396),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'log_count/WARNING': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 31, 4, 82405)}
Mon, 28 Dec 2015 15:32:09 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:33:09 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:33:10 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 3,
 'downloader/request_bytes': 663,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 33, 10, 70908),
 'log_count/DEBUG': 4,
 'log_count/INFO': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2015, 12, 28, 7, 32, 9, 818078)}
Mon, 28 Dec 2015 15:46:40 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:46:40 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 562,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 46, 40, 184605),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 46, 40, 50454)}
Mon, 28 Dec 2015 15:47:03 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:47:03 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2870,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 47, 3, 352138),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 47, 3, 16097)}
Mon, 28 Dec 2015 15:47:47 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:47:47 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2845,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 47, 47, 907605),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 47, 47, 591033)}
Mon, 28 Dec 2015 15:49:14 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:49:25 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2849,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 49, 25, 777038),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 49, 14, 848499)}
Mon, 28 Dec 2015 15:52:06 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:52:11 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 221,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2869,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 52, 11, 295418),
 'log_count/DEBUG': 2,
 'log_count/INFO': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2015, 12, 28, 7, 52, 6, 271970)}
Mon, 28 Dec 2015 15:52:56 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:54:15 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:55:38 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:56:43 logstats.py[line:47] INFO Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:57:59 logstats.py[line:47] INFO Crawled 25 pages (at 24 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:58:50 logstats.py[line:47] INFO Crawled 52 pages (at 27 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 15:58:50 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/exception_count': 4,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 4,
 'downloader/request_bytes': 19892,
 'downloader/request_count': 56,
 'downloader/request_method_count/GET': 56,
 'downloader/response_bytes': 1427882,
 'downloader/response_count': 52,
 'downloader/response_status_count/200': 52,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2015, 12, 28, 7, 58, 50, 517519),
 'log_count/DEBUG': 57,
 'log_count/INFO': 4,
 'request_depth_max': 2,
 'response_received_count': 52,
 'scheduler/dequeued': 56,
 'scheduler/dequeued/memory': 56,
 'scheduler/enqueued': 56,
 'scheduler/enqueued/memory': 56,
 'spider_exceptions/IndexError': 4,
 'start_time': datetime.datetime(2015, 12, 28, 7, 55, 38, 20097)}
Mon, 28 Dec 2015 15:59:36 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 16:00:08 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 38273,
 'downloader/request_count': 107,
 'downloader/request_method_count/GET': 107,
 'downloader/response_bytes': 287636,
 'downloader/response_count': 107,
 'downloader/response_status_count/200': 107,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2015, 12, 28, 8, 0, 8, 6628),
 'log_count/DEBUG': 108,
 'log_count/INFO': 1,
 'request_depth_max': 107,
 'response_received_count': 107,
 'scheduler/dequeued': 107,
 'scheduler/dequeued/memory': 107,
 'scheduler/enqueued': 108,
 'scheduler/enqueued/memory': 108,
 'start_time': datetime.datetime(2015, 12, 28, 7, 59, 36, 749494)}
Mon, 28 Dec 2015 16:00:11 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 16:00:20 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 10228,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 77536,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2015, 12, 28, 8, 0, 20, 401051),
 'log_count/DEBUG': 30,
 'log_count/INFO': 1,
 'request_depth_max': 29,
 'response_received_count': 29,
 'scheduler/dequeued': 29,
 'scheduler/dequeued/memory': 29,
 'scheduler/enqueued': 30,
 'scheduler/enqueued/memory': 30,
 'start_time': datetime.datetime(2015, 12, 28, 8, 0, 11, 868773)}
Mon, 28 Dec 2015 16:05:05 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 16:06:05 logstats.py[line:47] INFO Crawled 204 pages (at 204 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 16:06:48 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 119859,
 'downloader/request_count': 333,
 'downloader/request_method_count/GET': 333,
 'downloader/response_bytes': 896400,
 'downloader/response_count': 333,
 'downloader/response_status_count/200': 333,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2015, 12, 28, 8, 6, 48, 864949),
 'log_count/DEBUG': 334,
 'log_count/INFO': 2,
 'request_depth_max': 333,
 'response_received_count': 333,
 'scheduler/dequeued': 333,
 'scheduler/dequeued/memory': 333,
 'scheduler/enqueued': 334,
 'scheduler/enqueued/memory': 334,
 'start_time': datetime.datetime(2015, 12, 28, 8, 5, 5, 19670)}
Mon, 28 Dec 2015 16:07:32 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 16:07:46 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 15598,
 'downloader/request_count': 44,
 'downloader/request_method_count/GET': 44,
 'downloader/response_bytes': 117925,
 'downloader/response_count': 44,
 'downloader/response_status_count/200': 44,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2015, 12, 28, 8, 7, 46, 863286),
 'log_count/DEBUG': 45,
 'log_count/INFO': 1,
 'request_depth_max': 44,
 'response_received_count': 44,
 'scheduler/dequeued': 44,
 'scheduler/dequeued/memory': 44,
 'scheduler/enqueued': 45,
 'scheduler/enqueued/memory': 45,
 'start_time': datetime.datetime(2015, 12, 28, 8, 7, 32, 675432)}
Mon, 28 Dec 2015 16:09:12 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 16:09:13 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 16:11:01 logstats.py[line:47] INFO Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
Mon, 28 Dec 2015 16:11:18 statscollectors.py[line:47] INFO Dumping Scrapy stats:
{'downloader/request_bytes': 19894,
 'downloader/request_count': 56,
 'downloader/request_method_count/GET': 56,
 'downloader/response_bytes': 150185,
 'downloader/response_count': 56,
 'downloader/response_status_count/200': 56,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2015, 12, 28, 8, 11, 18, 87469),
 'log_count/DEBUG': 57,
 'log_count/INFO': 1,
 'request_depth_max': 56,
 'response_received_count': 56,
 'scheduler/dequeued': 56,
 'scheduler/dequeued/memory': 56,
 'scheduler/enqueued': 57,
 'scheduler/enqueued/memory': 57,
 'start_time': datetime.datetime(2015, 12, 28, 8, 11, 1, 626883)}
